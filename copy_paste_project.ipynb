{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d8e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random, sys, pyclipper\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea9537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_clip(anno_list, dev=False) -> list:\n",
    "    new_anno = anno_list[-1].copy()\n",
    "    new_anno_list = []\n",
    "    clip = [np.array(x) for x in segmentation_to_points(new_anno['segmentation'])]\n",
    "    for anno in anno_list[:-1]:\n",
    "        anno = anno.copy()\n",
    "        subj = [np.array(x) for x in segmentation_to_points(anno['segmentation'])]\n",
    "        for area in clip:\n",
    "            pc = pyclipper.Pyclipper()\n",
    "            pc.AddPath(area, pyclipper.PT_CLIP, True)\n",
    "            try:\n",
    "                pc.AddPaths(subj, pyclipper.PT_SUBJECT, True)\n",
    "            except:\n",
    "                if dev:\n",
    "                    print(anno_list)\n",
    "                    print('anno: ', anno)\n",
    "                    print(\"subj: \", subj)\n",
    "            subj = pc.Execute(pyclipper.CT_DIFFERENCE, pyclipper.PFT_EVENODD, pyclipper.PFT_EVENODD)\n",
    "            if subj == []:  \n",
    "                break\n",
    "        if subj == []:\n",
    "            continue\n",
    "        else:\n",
    "            anno['bbox'] = points_to_bbox(subj)\n",
    "            anno[\"segmentation\"] = points_to_segmentation(subj)\n",
    "            anno['area'] = points_to_area(subj)\n",
    "            new_anno_list.append(anno)\n",
    "    new_anno_list.append(new_anno)\n",
    "    return new_anno_list\n",
    "\n",
    "\n",
    "def flip_bbox(bbox: list, width: int) -> list:\n",
    "    \"\"\"\n",
    "    return a flipped bounding box according to the original bounding box list\n",
    "    and the width of the image.\n",
    "\n",
    "    Parameters:\n",
    "    bbox    : list    , the list presentation for the original bounding box\n",
    "    width   : integer , the width of the original image for flipping\n",
    "    \"\"\"\n",
    "    new_bbox = bbox.copy()\n",
    "    new_bbox[0] = width - new_bbox[0] - new_bbox[2]\n",
    "    return new_bbox\n",
    "\n",
    "def flip_segmentation(segmentation, width):\n",
    "    \"\"\"\n",
    "    return a flipped segmentation list or dictionary according to the original\n",
    "    segmentation instance and the width of the image.\n",
    "\n",
    "    Parameters:\n",
    "    segmentation  : list / dict , the list/dict presentation for the original\n",
    "                                  segmentation area\n",
    "    width         : integer     , the width of the original image for flipping\n",
    "    \"\"\"\n",
    "    if len(segmentation) == 0:\n",
    "        return []\n",
    "    if type(segmentation) == type({}):  # RLE representation\n",
    "        [height, width] = segmentation['size']  # parse the size of the image\n",
    "        new_count = []\n",
    "        count_sum = 0\n",
    "        for idx, count in enumerate(segmentation['counts']):\n",
    "            if idx % 2 == 0:\n",
    "                count_sum += count\n",
    "            if idx % 2 == 1:\n",
    "                count_sum += count\n",
    "                rows = count_sum // width\n",
    "                new_count.append((2 * rows + 1) * width - count_sum)\n",
    "                new_count.append(count)\n",
    "        if len(new_count) != len(segmentation['counts']):\n",
    "            new_count.append(height * width - sum(new_count))\n",
    "        assert (len(new_count) == len(segmentation['counts']))\n",
    "        return {'size': [height, width], 'counts': new_count}\n",
    "    else:  # Polygon representation\n",
    "        segmentations = []\n",
    "        for seg in segmentation:\n",
    "            new_segmentation = []\n",
    "            for idx, coordinate in enumerate(seg):\n",
    "                if idx % 2 == 0:\n",
    "                    coordinate = width - coordinate\n",
    "                new_segmentation.append(coordinate)\n",
    "            segmentations.append(new_segmentation)\n",
    "        return segmentations\n",
    "\n",
    "\n",
    "def wrap_coordinates_affine(points, M) -> list:\n",
    "    new_points = []\n",
    "    for point in points:\n",
    "        # new_points.append(M.dot(np.append(np.array(point), (1))))\n",
    "        temp = np.append(np.array(point), (1)).reshape(3, 1)\n",
    "        new_points.append(M.dot(temp).reshape(2))\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def bbox_to_points(bbox):\n",
    "    return np.array([\n",
    "        [0, 0],\n",
    "        [bbox[2], 0],\n",
    "        [bbox[2], bbox[3]],\n",
    "        [0, bbox[3]],\n",
    "    ])\n",
    "\n",
    "\n",
    "def segmentation_to_points(segmentation, bias=(0, 0)) -> list:\n",
    "    segmentation_points = []\n",
    "    for area in segmentation:\n",
    "        area_points = []\n",
    "        point = []\n",
    "        for idx, coord in enumerate(area):\n",
    "            if idx % 2 == 0:\n",
    "                point.append(coord - bias[0])\n",
    "            if idx % 2 == 1:\n",
    "                point.append(coord - bias[1])\n",
    "                area_points.append(point)\n",
    "                point = []\n",
    "        segmentation_points.append(area_points)\n",
    "    return segmentation_points\n",
    "\n",
    "\n",
    "def points_to_segmentation(segmentation_points) -> list:\n",
    "    segmentation = []\n",
    "    for area in segmentation_points:\n",
    "        segmentation_area = []\n",
    "        segmentation_xs = [point[0] for point in area]\n",
    "        segmentation_ys = [point[1] for point in area]\n",
    "        \n",
    "        for x, y in zip(segmentation_xs, segmentation_ys):\n",
    "            segmentation_area.append(x)\n",
    "            segmentation_area.append(y)\n",
    "\n",
    "        segmentation.append(segmentation_area)\n",
    "\n",
    "    return segmentation\n",
    "\n",
    "\n",
    "def points_to_bbox(segmentation_points) -> list:\n",
    "    segmentation_xs = []\n",
    "    segmentation_ys = []\n",
    "    for area in segmentation_points:\n",
    "        segmentation_xs += [point[0] for point in area]\n",
    "        segmentation_ys += [point[1] for point in area]\n",
    "    return [min(segmentation_xs), min(segmentation_ys), \n",
    "    max(segmentation_xs) - min(segmentation_xs), \n",
    "    max(segmentation_ys) - min(segmentation_ys)]\n",
    "\n",
    "\n",
    "def points_to_area(segmentation_points) -> float:\n",
    "    new_area = 0.0\n",
    "    for seg_area in segmentation_points:\n",
    "        new_area += Polygon(seg_area).area\n",
    "    return new_area\n",
    "    \n",
    "    \n",
    "def wrap_affine_transformation(bbox, segmentation, img, area, centre, angle=0.0, x_scale=1.0, y_scale=1.0, x_shift=0.0,\n",
    "                               y_shift=0.0, cols=0, rows=0, dev=False) -> np.array:\n",
    "    # defien return values\n",
    "    new_img = img.copy()\n",
    "    bias = [bbox[0], bbox[1]]\n",
    "    segmentation_points = segmentation_to_points(segmentation, bias)\n",
    "\n",
    "    # Transformation\n",
    "    M_transformation = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    try:\n",
    "        new_img = cv2.warpAffine(new_img, M_transformation, (2 * cols, 2 * rows))\n",
    "    except:\n",
    "        if dev:\n",
    "            print(\"scaling failed.\")\n",
    "            print('Feature shape: ', img.shape)\n",
    "            print('New feature shape: ', new_img.shape)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(new_img)\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    temp_seg_points = []\n",
    "    for area in segmentation_points:\n",
    "        temp_seg_points.append(wrap_coordinates_affine(area, M_transformation))\n",
    "    segmentation_points = temp_seg_points.copy()\n",
    "    centre = wrap_coordinates_affine([centre], M_transformation)[0]\n",
    "\n",
    "    # Scaling\n",
    "    try:\n",
    "        new_img = cv2.resize(new_img, None, fx=x_scale, fy=y_scale, interpolation=cv2.INTER_CUBIC)\n",
    "    except:\n",
    "        if dev:\n",
    "            print(\"scaling failed.\")\n",
    "            print('Feature shape: ', img.shape)\n",
    "            print('New feature shape: ', new_img.shape)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(new_img)\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    centre = [centre[0] * x_scale, centre[1] * y_scale]\n",
    "    temp_seg_points = []\n",
    "    for area in segmentation_points:\n",
    "        temp_seg_points.append([[x * x_scale, y * y_scale] for [x, y] in area])\n",
    "    segmentation_points = temp_seg_points.copy()\n",
    "    centre = wrap_coordinates_affine([centre], M_transformation)[0]\n",
    "\n",
    "    # Rotation\n",
    "    M_rotation = cv2.getRotationMatrix2D((centre[0], centre[1]), angle, 1)\n",
    "    try:\n",
    "        new_img = cv2.warpAffine(new_img, M_rotation, (2 * cols, 2 * rows))\n",
    "    except:\n",
    "        if dev:\n",
    "            print(\"Rotation failed.\")\n",
    "            print('Feature shape: ', img.shape)\n",
    "            print('New feature shape: ', new_img.shape)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(new_img)\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    temp_seg_points = []\n",
    "    for area in segmentation_points:\n",
    "        temp_seg_points.append(wrap_coordinates_affine(area, M_rotation))\n",
    "    segmentation_points = temp_seg_points.copy()\n",
    "\n",
    "    # Fix bbox and segmentation\n",
    "    subj = segmentation_points\n",
    "    clip = [[0, 0], [0, rows], [cols, rows], [cols, 0]]\n",
    "    pc = pyclipper.Pyclipper()\n",
    "    pc.AddPath(clip, pyclipper.PT_CLIP, True)\n",
    "    try:\n",
    "        pc.AddPaths(subj, pyclipper.PT_SUBJECT, True)\n",
    "    except:\n",
    "        if dev:\n",
    "            print(\"Clipping failed.\")\n",
    "            print(\"segmentation: \", segmentation)\n",
    "            print('subj: ', subj)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(new_img)\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    segmentation_points = pc.Execute(pyclipper.CT_INTERSECTION, pyclipper.PFT_EVENODD, pyclipper.PFT_EVENODD)\n",
    "    if segmentation_points == []:\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    temp_seg_points = []\n",
    "    for area in segmentation_points:\n",
    "        pco = pyclipper.PyclipperOffset()\n",
    "        pco.AddPath(area, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "        temp_seg_points.append(pco.Execute(-2.0))\n",
    "\n",
    "    if segmentation_points == []:\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    new_segmentation = points_to_segmentation(segmentation_points)\n",
    "    new_bbox = points_to_bbox(segmentation_points)\n",
    "    new_area = points_to_area(segmentation_points)\n",
    "\n",
    "    if new_area <= 40:\n",
    "        return (False, bbox, segmentation, img, area)\n",
    "    return (True, new_bbox, new_segmentation, new_img[:rows, :cols], new_area)\n",
    "\n",
    "\n",
    "def get_processed_image_dict(json_file, flip_image=False, remove_rle=True, dev=False) -> (dict, set):\n",
    "    \"\"\"\n",
    "    return a dictionary of pictures info and a set of original image ids:\n",
    "    used as the preparation for converting certain json files to acceptable\n",
    "    COCO instance datasets.\n",
    "\n",
    "    Parameters:\n",
    "    json_file   : string  , the path to the annotation file\n",
    "    percentage  : float   , percentage of copy-paste instance compared to the\n",
    "                            original instance\n",
    "    flip_image  : bool    , indicator for generating flipped image or not.\n",
    "    \"\"\"\n",
    "    # json_file = os.path.join(img_dir, 'annotations','instances_val2017.json')\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "        \n",
    "    img_dir = os.path.split(os.path.split(json_file)[0])[0]\n",
    "    \n",
    "    # Declarations of variables\n",
    "    anno_idx = 0\n",
    "    imageid_data_dict = {}\n",
    "    flipped_imageid_data_dict = {}\n",
    "    remove_set = set()\n",
    "    anno_list = imgs_anns['annotations']\n",
    "    category_list = [x['id'] for x in imgs_anns['categories']]\n",
    "    category_dict = {}\n",
    "    for idx, id in enumerate(category_list):\n",
    "      category_dict[id] = idx\n",
    "\n",
    "    ### dataset preprocessing and registration ###\n",
    "    for v in anno_list:\n",
    "        image_id = str(v['image_id'])\n",
    "        # Ignore images with RLE encoding if remove_rle is True\n",
    "        if remove_rle:\n",
    "            if v['iscrowd'] == 1:\n",
    "                remove_set.add(image_id)\n",
    "                if flip_image:\n",
    "                    remove_set.add(image_id + '_')\n",
    "                continue\n",
    "        # Add features to existing entities\n",
    "        if image_id in imageid_data_dict.keys() and image_id not in remove_set:\n",
    "            #### Update original image\n",
    "            imageid_data_dict[image_id]['annotations'].append({\n",
    "                \"bbox\": v['bbox'],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": v['segmentation'],\n",
    "                \"category_id\": category_dict[v['category_id']],\n",
    "                \"iscrowd\": v['iscrowd'],\n",
    "                'area': v['area'],\n",
    "                'image_id': image_id,\n",
    "                'id': anno_idx,\n",
    "            })\n",
    "            anno_idx += 1\n",
    "            if flip_image == True:\n",
    "                #### Update flipped image\n",
    "                width = imageid_data_dict[image_id + '_']['width']\n",
    "                imageid_data_dict[image_id + '_']['annotations'].append({\n",
    "                    \"bbox\": flip_bbox(v['bbox'], width),\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"segmentation\": flip_segmentation(v['segmentation'], width),\n",
    "                    \"category_id\": category_dict[v['category_id']],\n",
    "                    \"iscrowd\": v['iscrowd'],\n",
    "                    'area': v['area'],\n",
    "                    'image_id': image_id + '_',\n",
    "                    'id': anno_idx,\n",
    "                })\n",
    "                flipped_imageid_data_dict[image_id + '_']['annotations'].append(imageid_data_dict[image_id + '_']['annotations'][-1])\n",
    "                anno_idx += 1\n",
    "        elif image_id not in imageid_data_dict.keys() and image_id not in remove_set:\n",
    "            # format annotations for new images\n",
    "            record = {}\n",
    "\n",
    "            filename = os.path.join(\n",
    "                img_dir,\n",
    "                '0' * (12 - len(str(image_id))) + str(image_id) + '.jpg'\n",
    "            )\n",
    "            image = cv2.imread(filename)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            record[\"file_name\"] = filename\n",
    "            record[\"image_id\"] = image_id\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "            record[\"annotations\"] = [{\n",
    "                \"bbox\": v['bbox'],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": v['segmentation'],\n",
    "                \"category_id\": category_dict[v['category_id']],\n",
    "                \"iscrowd\": v['iscrowd'],\n",
    "                'area': v['area'],\n",
    "                'image_id': image_id,\n",
    "                'id': anno_idx,\n",
    "            }]\n",
    "            anno_idx += 1\n",
    "            imageid_data_dict[image_id] = record\n",
    "\n",
    "            if flip_image == True:\n",
    "                # generate and record derived pictures\n",
    "                new_name = os.path.join(\n",
    "                    img_dir,\n",
    "                    '0' * (12 - len(image_id)) + image_id + '_' + '.jpg'\n",
    "                )\n",
    "                image = cv2.flip(image, 1)\n",
    "                cv2.imwrite(new_name, image)\n",
    "\n",
    "                record_ = {}\n",
    "                record_[\"file_name\"] = new_name\n",
    "                record_[\"image_id\"] = image_id + '_'\n",
    "                record_[\"height\"] = height\n",
    "                record_[\"width\"] = width\n",
    "                record_[\"annotations\"] = [{\n",
    "                    \"bbox\": flip_bbox(v['bbox'], width),\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"segmentation\": flip_segmentation(v['segmentation'], width),\n",
    "                    \"category_id\": category_dict[v['category_id']],\n",
    "                    \"iscrowd\": v['iscrowd'],\n",
    "                    'area': v['area'],\n",
    "                    'image_id': image_id + '_',\n",
    "                    'id': anno_idx,\n",
    "                }]\n",
    "                anno_idx += 1\n",
    "                imageid_data_dict[image_id + '_'] = record_\n",
    "                flipped_imageid_data_dict[image_id + '_'] = record_\n",
    "\n",
    "                if dev:\n",
    "                    if anno_idx >= 10:\n",
    "                        break\n",
    "    for image_id in remove_set:\n",
    "        if image_id in imageid_data_dict.keys():\n",
    "            imageid_data_dict.pop(image_id)\n",
    "            if flip_image and image_id[-1] == '_':\n",
    "                flipped_imageid_data_dict.pop(image_id)\n",
    "    ### End of dataset preprocessing and registration ###\n",
    "    original_imageid_set = set([str.strip(key, '_') for key in imageid_data_dict.keys()])\n",
    "    print(\"End of annotation preprocessing.\")\n",
    "    return (imageid_data_dict, original_imageid_set, flipped_imageid_data_dict)\n",
    "\n",
    "def cp_train_test_split(imageid_data_dict, original_imageid_set, train_percentage=0.7, test_percentage=0.2,\n",
    "                        val_percentage=0.1):\n",
    "    train_list, test_list = train_test_split(list(original_imageid_set), train_size=train_percentage,\n",
    "                                             test_size=test_percentage + val_percentage)\n",
    "    t = int(len(test_list) * test_percentage / (test_percentage + val_percentage))\n",
    "    test_list, val_list = (test_list[: t], test_list[t:])\n",
    "    if train_list[0] + '_' in imageid_data_dict.keys():\n",
    "        return [imageid_data_dict[id] for id in train_list] + [imageid_data_dict[id + '_'] for id in train_list], \\\n",
    "               [imageid_data_dict[id] for id in test_list] + [imageid_data_dict[id + '_'] for id in test_list], \\\n",
    "               [imageid_data_dict[id] for id in val_list] + [imageid_data_dict[id + '_'] for id in val_list]\n",
    "    return [imageid_data_dict[id] for id in train_list], [imageid_data_dict[id] for id in test_list], [\n",
    "        imageid_data_dict[id] for id in val_list]\n",
    "\n",
    "def copy_paste_augmentation(filename, image_dicts_list, percentage=0.0, scaling=True, rotation=True, \n",
    "                            keep_aspect_ratio=False, dev=False) -> list:\n",
    "    img_dir = os.path.split(os.path.split(filename)[0])[0]\n",
    "    decorator = '_'\n",
    "    if scaling:\n",
    "        decorator += 's'\n",
    "    if rotation:\n",
    "        decorator += 'r'\n",
    "    if keep_aspect_ratio:\n",
    "        decorator += 'a'\n",
    "    print(\"start Copy Paste Augmentation, decorator = \", decorator)\n",
    "    ### Randomly adopting copy paste augmentation ###\n",
    "    anno_idx = -1\n",
    "    new_image_dict_list = image_dicts_list.copy()\n",
    "    if percentage > 0.000001:\n",
    "        count = 0\n",
    "        maximum = int(len(image_dicts_list) * percentage)\n",
    "        if dev:\n",
    "            maximum = 3\n",
    "        original_length = len(image_dicts_list)\n",
    "        while (count < maximum):\n",
    "            # get random feature image\n",
    "            dict_idx_feature = random.randint(0, original_length - 1)\n",
    "            feature_dict = image_dicts_list[dict_idx_feature]\n",
    "            feature_img = cv2.imread(feature_dict['file_name'])\n",
    "            feature_anno = feature_dict['annotations']\n",
    "\n",
    "            if dev:\n",
    "                print(\"feature image:\")\n",
    "                plt.imshow(feature_img)\n",
    "\n",
    "            for feature in feature_anno:\n",
    "                if type(feature['segmentation']) == type([]) and len(feature['segmentation']) == 1:\n",
    "                    bbox = np.array(feature['bbox'], dtype=np.int32)\n",
    "\n",
    "                    segmentation_points = segmentation_to_points(feature['segmentation'], [0, 0])\n",
    "                    feature_mask = np.zeros((feature_dict['height'], feature_dict['width']))\n",
    "                    cv2.fillPoly(feature_mask, np.array(segmentation_points, dtype=np.int32), 1)\n",
    "\n",
    "                    # get random background image\n",
    "                    dict_idx_background = random.randint(0, original_length - 1)\n",
    "                    background_dict = image_dicts_list[dict_idx_background]\n",
    "                    background_img = cv2.imread(background_dict['file_name'])\n",
    "                    background_anno = background_dict['annotations']\n",
    "\n",
    "                    if dev:\n",
    "                        print(\"background image:\")\n",
    "                        plt.imshow(background_img)\n",
    "\n",
    "                    feature_mask = feature_mask.astype(bool)\n",
    "                    feature_block = (feature_img * np.stack([feature_mask] * 3, axis=2))[bbox[1]:bbox[1] + bbox[3],\n",
    "                                    bbox[0]:bbox[0] + bbox[2]]\n",
    "\n",
    "                    validity = False\n",
    "                    try_out_time = 2\n",
    "                    while (not validity and try_out_time):\n",
    "                        angle = 0\n",
    "                        x_scale = 1\n",
    "                        y_scale = 1\n",
    "                        x_shift = random.random() * (background_dict['width'] - feature_block.shape[1])\n",
    "                        y_shift = random.random() * (background_dict['height'] - feature_block.shape[2])\n",
    "                        if scaling:\n",
    "                            x_scale = random.random() * 2.7 + 0.3\n",
    "                            y_scale = random.random() * 2.7 + 0.3\n",
    "                            if keep_aspect_ratio:\n",
    "                                y_scale = x_scale\n",
    "                        if rotation:\n",
    "                            angle = random.random() * 360\n",
    "                        (validity,\n",
    "                        new_bbox,\n",
    "                        new_segmentation,\n",
    "                        affine_feature,\n",
    "                        new_area) = wrap_affine_transformation(\n",
    "                                                    feature['bbox'],\n",
    "                                                    feature['segmentation'],\n",
    "                                                    feature_block.copy(),\n",
    "                                                    feature['area'],\n",
    "                                                    centre=[feature_block.shape[1] / 2, feature_block.shape[0] / 2],\n",
    "                                                    angle=angle,\n",
    "                                                    x_scale=x_scale,\n",
    "                                                    y_scale=y_scale,\n",
    "                                                    x_shift=x_shift,\n",
    "                                                    y_shift=y_shift,\n",
    "                                                    cols=background_dict['width'],\n",
    "                                                    rows=background_dict['height'])\n",
    "                        try_out_time -= 1\n",
    "                    if try_out_time == 0:\n",
    "                      continue\n",
    "\n",
    "                    bg_mask = np.zeros((background_dict['height'], background_dict['width']))\n",
    "                    new_segmentation_points = segmentation_to_points(new_segmentation, [0, 0])\n",
    "                    try:\n",
    "                        cv2.fillPoly(bg_mask, [np.array(x) for x in new_segmentation_points], 1)\n",
    "                    except ValueError:\n",
    "                        print(\"handle value error for cv2.fillpoly\", new_segmentation_points)\n",
    "                        plt.imshow(feature_block)\n",
    "                        plt.imshow(affine_feature)\n",
    "                        plt.imshow(bg_mask)\n",
    "                    bg_mask = bg_mask.astype(bool)\n",
    "                    if dev:\n",
    "                        s1 = affine_feature * np.stack([bg_mask] * 3, axis=2) + background_img * np.stack(\n",
    "                            [~bg_mask] * 3, axis=2)\n",
    "\n",
    "                    kernel = np.ones((3, 3), np.uint8)\n",
    "                    bg_mask = cv2.erode(np.array(bg_mask).astype(np.float32), kernel, iterations=1).astype(bool)\n",
    "                    sythesised_img = affine_feature * np.stack([bg_mask] * 3, axis=2) + background_img * np.stack(\n",
    "                        [~bg_mask] * 3, axis=2)\n",
    "\n",
    "                    if dev:\n",
    "                        plt.imshow(feature_block)\n",
    "                        plt.imshow(affine_feature)\n",
    "                        plt.imshow(s1)\n",
    "                        plt.imshow(sythesised_img)\n",
    "                    \n",
    "                    image_id = background_dict['image_id'] + decorator + str(anno_idx)\n",
    "                    new_name = os.path.join(\n",
    "                        img_dir,\n",
    "                        '0' * (12 - len(image_id)) + image_id + '.jpg'\n",
    "                    )\n",
    "                    cv2.imwrite(new_name, sythesised_img)\n",
    "\n",
    "                    record = {}\n",
    "                    record[\"file_name\"] = new_name\n",
    "                    record[\"image_id\"] = image_id\n",
    "                    record[\"height\"] = background_dict['height']\n",
    "                    record[\"width\"] = background_dict['width']\n",
    "                    record[\"annotations\"] = background_anno.copy()\n",
    "                    record[\"annotations\"].append({\n",
    "                        \"bbox\": new_bbox,\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": new_segmentation,\n",
    "                        \"category_id\": feature['category_id'],\n",
    "                        \"iscrowd\": feature['iscrowd'],\n",
    "                        'area': new_area,\n",
    "                        'image_id': image_id,\n",
    "                        'id': anno_idx,\n",
    "                    })\n",
    "                    record[\"annotations\"] = cp_clip(record[\"annotations\"])\n",
    "                    anno_idx -= 1\n",
    "                    new_image_dict_list.append(record)\n",
    "                    count += 1\n",
    "\n",
    "                    if dev:\n",
    "                        img = sythesised_img\n",
    "                        visualizer = Visualizer(img[:, :, ::-1], scale=0.5)\n",
    "                        out = visualizer.draw_dataset_dict(record)\n",
    "                        plt.imshow(out.get_image()[:, :, ::-1])\n",
    "                else:\n",
    "                  continue\n",
    "\n",
    "    ### End of Randomly adopting copy paste augmentation ###\n",
    "    return new_image_dict_list\n",
    "\n",
    "\n",
    "def get_list(path, work_list):\n",
    "    return work_list\n",
    "\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "\n",
    "\n",
    "def train(training_set_name, test_set_name):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.DATASETS.TRAIN = (test_set_name,)\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") \n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "    cfg.SOLVER.BASE_LR = 0.00025\n",
    "    cfg.SOLVER.MAX_ITER = 20\n",
    "    cfg.SOLVER.STEPS = []  # do not decay learning rate\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\n",
    "    cfg.OUTPUT_DIR = os.path.join(res_dir, 'output', training_set_name)\n",
    "    cfg.DATASETS.TEST = (test_set_name,)\n",
    "    cfg.TEST.EVAL_PERIOD = 2\n",
    "    if torch.backends.mps.is_available():\n",
    "        cfg.MODEL.DEVICE = \"mps\"\n",
    "    elif not torch.cuda.is_available():\n",
    "        cfg.MODEL.DEVICE = \"cpu\"\n",
    "      \n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    trainer = MyTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    print('Start training: ', training_set_name)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a61c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset: ...\n",
      "End of annotation preprocessing.\n",
      "Original dataset prepared:  4541  images.\n",
      "Coco training dataset prepared:  3632  images.\n",
      "End of annotation preprocessing.\n",
      "Flip Augmented dataset prepared:  3632  images.\n",
      "2157 2157_\n",
      "start Copy Paste Augmentation, decorator =  _sr\n"
     ]
    }
   ],
   "source": [
    "train_img_dir = '/Volumes/Info/cp_data/val2017'\n",
    "test_img_dir = '/Volumes/Info/cp_data/val2017'\n",
    "res_dir = '/Volumes/result'\n",
    "train_file = os.path.join(train_img_dir, 'annotations', 'instances_train2017.json')\n",
    "\n",
    "if 'test' == 'test':\n",
    "    train_img_dir = os.path.join(test_img_dir)\n",
    "    train_file = os.path.join(test_img_dir, 'annotations', 'instances_val2017.json')\n",
    "\n",
    "test_img_dir = os.path.join(test_img_dir)\n",
    "test_file = os.path.join(test_img_dir, 'annotations', 'instances_val2017.json')\n",
    "\n",
    "with open(train_file) as f:\n",
    "    imgs_anns = json.load(f)\n",
    "thing_classes = [x['name'] for x in imgs_anns['categories']]\n",
    "\n",
    "print('Preparing dataset: ...')\n",
    "original_dataset_dict, id_set, _ = get_processed_image_dict(train_file, flip_image=False, remove_rle=True)\n",
    "original_dataset_list = list(original_dataset_dict.values())\n",
    "train_dataset_list, test_list = train_test_split(original_dataset_list, train_size=0.8, test_size=0.2, random_state = 0)\n",
    "\n",
    "print('Original dataset prepared: ', len(original_dataset_list), ' images.')\n",
    "print('Coco training dataset prepared: ', len(train_dataset_list), ' images.')\n",
    "\n",
    "_, _, flipped_dataset = get_processed_image_dict(train_file, flip_image=True, remove_rle=True)\n",
    "flipped_dataset_list = list(flipped_dataset.values())\n",
    "flipped_dataset_list, _ = train_test_split(flipped_dataset_list, train_size=0.8, test_size=0.2, random_state = 0)\n",
    "\n",
    "print('Flip Augmented dataset prepared: ', len(flipped_dataset_list), ' images.')\n",
    "\n",
    "print(train_dataset_list[0]['image_id'],flipped_dataset_list[0]['image_id'] )\n",
    "# assert(train_dataset_list[0]['image_id'] + '_' == flipped_dataset_list[0]['image_id'])\n",
    "\n",
    "cp_train_list = copy_paste_augmentation(train_file, train_dataset_list, percentage=1.0, scaling=True,\n",
    "                                            rotation=True, dev=False)\n",
    "print('Copy paste fully augmented dataset prepared: ', len(cp_train_list), ' images.')\n",
    "\n",
    "cp_only_train_list = cp_train_list[len(train_dataset_list):]\n",
    "print('Copy paste only, fully augmented dataset prepared: ', len(cp_only_train_list), ' images.')\n",
    "\n",
    "scaling_train_list = copy_paste_augmentation(train_file, train_dataset_list, percentage=1.0, scaling=True,\n",
    "                                              rotation=False, dev=False)\n",
    "scaling_train_list = scaling_train_list[len(train_dataset_list):]\n",
    "print('Copy paste only, scaling augmented dataset prepared: ', len(scaling_train_list), ' images.')\n",
    "\n",
    "rotation_train_list = copy_paste_augmentation(train_file, train_dataset_list, percentage=1.0, scaling=False,\n",
    "                                              rotation=True, dev=False)\n",
    "rotation_train_list = rotation_train_list[len(train_dataset_list):]\n",
    "print('Copy paste only, rotation augmented dataset prepared: ', len(rotation_train_list), ' images.')\n",
    "\n",
    "print('Test dataset prepared: ', len(test_list), ' images.')\n",
    "\n",
    "dataset_name_list = [\"co_train\", \"flip_train\", \"cp_train\", \\\n",
    "                \"cp_only_train\", \"scale_train\", \"rotation_train\", \"co_test\"]\n",
    "\n",
    "dataset_list = [\n",
    "                train_dataset_list, \\\n",
    "                flipped_dataset_list, \\\n",
    "                cp_train_list, \\\n",
    "                cp_only_train_list, \\\n",
    "                scaling_train_list, \\\n",
    "                rotation_train_list, \\\n",
    "                test_list\\\n",
    "                ]\n",
    "\n",
    "for name in dataset_name_list:\n",
    "    if name in DatasetCatalog:\n",
    "        DatasetCatalog.remove(name)\n",
    "\n",
    "for idx, name in enumerate(dataset_name_list):\n",
    "    DatasetCatalog.register(name, lambda res_dir=res_dir: get_list(res_dir, dataset_list[idx]))\n",
    "\n",
    "for name in dataset_name_list:\n",
    "    MetadataCatalog.get(name).set(thing_classes=thing_classes)\n",
    "\n",
    "print('All dataset registered.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e32dd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4,figsize=(200, 200))\n",
    "print(\"Fully Augmented methodology: Transformation, Rotation, Scaling. \\n\")\n",
    "print('Raw picture                    Augmented picture            Raw segmentations             New segmentations')\n",
    "axs[0,0].set_title('raw picture')\n",
    "axs[0,1].set_title('augmented picture')\n",
    "axs[0,2].set_title('raw segmentation')\n",
    "axs[0,3].set_title('new segmentation')\n",
    "row = 0\n",
    "for axl in axs:\n",
    "    for ax in axl:\n",
    "        ax.axis('off')\n",
    "\n",
    "for d in random.sample(cp_only_train_list, 3):\n",
    "    original_d = original_dataset_dict[d['image_id'].split('_')[0]]\n",
    "    original_img = cv2.imread(original_d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(original_img[:, :, ::-1], scale=0.5)\n",
    "    original_out = visualizer.draw_dataset_dict(original_d)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    axs[row, 0].imshow(original_img)\n",
    "    axs[row, 1].imshow(img)\n",
    "    axs[row, 2].imshow(original_out.get_image()[:, :, ::-1])\n",
    "    axs[row, 3].imshow(out.get_image()[:, :, ::-1])\n",
    "    row += 1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103feb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4,figsize=(200, 200))\n",
    "print(\"Partially Augmented methodology: Transformation, Scaling. \\n\")\n",
    "print('Raw picture                    Augmented picture            Raw segmentations             New segmentations')\n",
    "axs[0,0].set_title('raw picture')\n",
    "axs[0,1].set_title('augmented picture')\n",
    "axs[0,2].set_title('raw segmentation')\n",
    "axs[0,3].set_title('new segmentation')\n",
    "row = 0\n",
    "for axl in axs:\n",
    "    for ax in axl:\n",
    "        ax.axis('off')\n",
    "\n",
    "for d in random.sample(scaling_train_list, 3):\n",
    "    original_d = original_dataset_dict[d['image_id'].split('_')[0]]\n",
    "    original_img = cv2.imread(original_d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(original_img[:, :, ::-1], scale=0.5)\n",
    "    original_out = visualizer.draw_dataset_dict(original_d)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    axs[row, 0].imshow(original_img)\n",
    "    axs[row, 1].imshow(img)\n",
    "    axs[row, 2].imshow(original_out.get_image()[:, :, ::-1])\n",
    "    axs[row, 3].imshow(out.get_image()[:, :, ::-1])\n",
    "    row += 1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e32b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4,figsize=(200, 200))\n",
    "print(\"Partially Augmented methodology: Transformation, Rotation. \\n\")\n",
    "print('Raw picture                    Augmented picture            Raw segmentations             New segmentations')\n",
    "axs[0,0].set_title('raw picture')\n",
    "axs[0,1].set_title('augmented picture')\n",
    "axs[0,2].set_title('raw segmentation')\n",
    "axs[0,3].set_title('new segmentation')\n",
    "row = 0\n",
    "for axl in axs:\n",
    "    for ax in axl:\n",
    "        ax.axis('off')\n",
    "\n",
    "for d in random.sample(rotation_train_list, 3):\n",
    "    original_d = original_dataset_dict[d['image_id'].split('_')[0]]\n",
    "    original_img = cv2.imread(original_d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(original_img[:, :, ::-1], scale=0.5)\n",
    "    original_out = visualizer.draw_dataset_dict(original_d)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    axs[row, 0].imshow(original_img)\n",
    "    axs[row, 1].imshow(img)\n",
    "    axs[row, 2].imshow(original_out.get_image()[:, :, ::-1])\n",
    "    axs[row, 3].imshow(out.get_image()[:, :, ::-1])\n",
    "    row += 1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22844ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "            \"co_train\", \\\n",
    "            \"cp_only_train\",\\\n",
    "            \"scale_train\", \\\n",
    "            \"rotation_train\",\\\n",
    "                \"cp_train\", \\\n",
    "            \"flip_train\"\n",
    "            ]\n",
    "\n",
    "for name in test_list:\n",
    "    train(name, \"co_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
